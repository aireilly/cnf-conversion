[id="cnf-best-practices-nics"]
= NICs

Intel Fortville XXV710 â€“ 25G::
The XXV710 is a PCI Express 3.0, dual port 25 Gigabit Ethernet controller targeted for system configurations such as rack mounted or pedestal servers, where it can be used as an add-on NIC.

Intel i40e driver::
The ixgbe driver supports XXV710-based PCI Express 25 Gigabit network adaptors that support the IEEE 802.3by network specification. These drivers are only supported as a loadable module at this time. Intel is not supplying patches against the kernel source to allow for static linking of the driver.

The following features are now available in supported kernels:

* Native VLANs
* Channel bonding (teaming)
* SNMP
* Generic Receive Offload
* Data Center Bridging

Intel i40evf driver::
The i40e module is required in guest containers to enable enhanced networking support, SR-IOV. The i40e driver supports Intel XXV710 virtual function (VF) devices and can only be activated on kernels that support SR-IOV. The i40evf driver supports virtual functions generated by the ixgbe driver installed in the compute node host OS.

Intel i40evf driver versions::
A correct version of this driver is most likely included in your guest OS distribution. Even if your distribution contains a version of the driver it may not work with the Verizon install XXV710, as earlier versions of the driver did not support the 25GbE standard. Verizon recommends using the latest version of the driver supported by your guest OS distribution kernel version. A list of Intel driver versions and driver source code downloads can be found here.

link:https://downloadcenter.intel.com/download/22283/Intel-Ethernet-Adapter-Complete-Driver-Pack?product=95259[]

.VCP CNF requirement
[IMPORTANT]
====
If CNF is to be deployed in Webscale Edge environment and needs to use SR-IOV the CNF must support the Intel i40evf driver - The NIC cards for SR-IOV are Intel XXV710
====

Nvidia Mellanox CX-5::
The Mellanox Nvidia CX-5 can be used for OVS Hardware Offload which offers SR-IOV like performance by directly connecting VMs with the physical interfaces using a Virtual Function (VF).

Driver information for the Mellanox Nvidia can be found here:

link:https://network.nvidia.com/products/ethernet-drivers/linux/mlnx_en/[]

